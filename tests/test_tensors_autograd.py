import unittest
import torch
from pyhilbert.tensors import Tensor

class TestTensorAutograd(unittest.TestCase):
    def test_tensor_autograd_behavior(self):
        # Setup basic data
        dims = ()  # Empty dimensions for simplicity
        data = torch.randn(3, 3)
        
        # --- 1. Test requires_grad property ---
        t_false = Tensor(data=data.clone(), dims=dims)
        self.assertFalse(t_false.requires_grad, "Should be False when data does not require grad")
        
        t_true = Tensor(data=data.clone().requires_grad_(True), dims=dims)
        self.assertTrue(t_true.requires_grad, "Should be True when data requires grad")

        # --- 2. Test attach() ---
        # Case A: attach() on a tensor without grad
        t_attached = t_false.attach()
        self.assertIsNot(t_attached, t_false)
        self.assertTrue(t_attached.requires_grad)
        self.assertTrue(t_attached.data.is_leaf)
        
        # Verify independence: modifying original shouldn't affect attached
        t_false.data[0, 0] += 100
        self.assertNotEqual(t_attached.data[0, 0], t_false.data[0, 0], "Attached tensor should be a deep copy/clone")

        # Case B: attach() on a tensor ALREADY with grad
        t_reattached = t_true.attach()
        self.assertIs(t_reattached, t_true, "Should return self if already attached")

        # --- 3. Test detach() ---
        # Should return NEW tensor, grad=False, SHARED storage
        t_detached = t_true.detach()
        self.assertIsNot(t_detached, t_true)
        self.assertFalse(t_detached.requires_grad)
        
        # Verify shared storage: modifying detached data SHOULD affect original
        # (Note: we modify the detached one to avoid autograd errors on leaf variables)
        original_val = t_true.data[0, 0].item()
        t_detached.data[0, 0] += 50
        self.assertEqual(t_true.data[0, 0], t_detached.data[0, 0], "Detached tensor should share storage")
        self.assertNotEqual(t_true.data[0, 0], original_val)

        # --- 4. Test clone() ---
        t_cloned = t_true.clone()
        self.assertIsNot(t_cloned, t_true)
        self.assertEqual(t_cloned.requires_grad, t_true.requires_grad)
        
        # Verify independence
        t_cloned.data[1, 1] += 200
        self.assertNotEqual(t_true.data[1, 1], t_cloned.data[1, 1], "Cloned tensor should have independent storage")

    def test_autograd_application_flow(self):
        """
        Test a simulated application flow involving forward pass, 
        gradient calculation, and detachment.
        """
        # 1. Simulate inputs and weights
        dims = ()
        x_data = torch.tensor([1.0, 2.0, 3.0])
        w_data = torch.tensor([0.5, 0.5, 0.5])
        
        # Create tensors
        x = Tensor(data=x_data, dims=dims)
        w = Tensor(data=w_data, dims=dims)
        
        # 2. Attach gradients
        x_input = x.attach()
        w_param = w.attach()
        
        # 3. Forward pass: y = x + w (using operator_add)
        y = x_input + w_param
        
        # 4. Compute loss (scalar)
        # Target: [2.0, 3.0, 4.0]
        target = torch.tensor([2.0, 3.0, 4.0])
        diff = y.data - target
        loss = (diff ** 2).sum()
        
        # 5. Backward pass
        loss.backward()
        
        # 6. Verify gradients
        # y = x + w
        # L = sum((y - t)^2) = sum((x + w - t)^2)
        # dL/dx = 2 * (x + w - t)
        # x=[1,2,3], w=[.5,.5,.5], t=[2,3,4]
        # x+w-t = [1.5, 2.5, 3.5] - [2, 3, 4] = [-0.5, -0.5, -0.5]
        # grad = 2 * (-0.5) = -1.0
        
        expected_grad = torch.tensor([-1.0, -1.0, -1.0])
        
        self.assertTrue(torch.allclose(w_param.data.grad, expected_grad),
                        f"Expected grad {expected_grad}, got {w_param.data.grad}")
        self.assertTrue(torch.allclose(x_input.data.grad, expected_grad))
        
        # 7. Test Detach
        w_detached = w_param.detach()
        y_val = x_input + w_detached
        loss_val = y_val.data.sum()
        
        # Zero gradients
        if x_input.data.grad is not None:
            x_input.data.grad.zero_()
            
        loss_val.backward()
        
        # x_input should have grad 1.0 (derivative of sum(x))
        self.assertTrue(torch.allclose(x_input.data.grad, torch.ones(3)))
        
        # w_detached should not have grad
        self.assertFalse(w_detached.requires_grad)
        self.assertIsNone(w_detached.data.grad)

    def test_clone_autograd(self):
        """Test that clone() preserves autograd history."""
        dims = ()
        x = Tensor(data=torch.tensor([1.0]), dims=dims).attach()
        
        # Clone
        y = x.clone()
        
        # Operation
        z = y.data * 2
        loss = z.sum()
        loss.backward()
        
        # d(2*y)/dx = 2 * dy/dx = 2 * 1 = 2
        self.assertEqual(x.data.grad.item(), 2.0)

if __name__ == '__main__':
    unittest.main()
